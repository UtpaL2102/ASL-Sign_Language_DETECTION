# ASL-Sign_Language_DETECTION

# Project Overview
This project focuses on the development of a Machine Learning model designed to detect American Sign Language (ASL) signs shown in live camera feeds. The model has been trained on a dataset comprising ASL images, with dedicated sets for training, testing, and validation.

# Model Development
Notebook: Model.ipynb
In this notebook, the model architecture was crafted, trained, and fine-tuned. The steps included:
Data preprocessing and augmentation techniques for robust learning.
Building the neural network architecture optimized for ASL sign recognition.
Training the model using the designated training set.
Validation and fine-tuning for optimal performance.
Saving the trained model for later use


In the Notebook: Sign=Language-detection.ipynb Live Detection is done:
Real-time Processing: Efficient video processing techniques for live camera input.
Model Inference: Utilizing the pre-trained model to classify signs in each frame.
Display Output: Showing the detected sign with associated information on the live feed.
