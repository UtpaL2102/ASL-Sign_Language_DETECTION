# ASL-Sign_Language_DETECTION

# Project Overview
This project focuses on the development of a Machine Learning model designed to detect American Sign Language (ASL) signs shown in live camera feeds. The model has been trained on a dataset comprising ASL images, with dedicated sets for training, testing, and validation.

# Model Development
Notebook: Model.ipynb
In this notebook, the model architecture was crafted, trained, and fine-tuned. The steps included:
Data preprocessing and augmentation techniques for robust learning.
Building the neural network architecture optimized for ASL sign recognition.
Training the model using the designated training set.
Validation and fine-tuning for optimal performance.
Saving the trained model for later use

# Model Building
Notebook: 
Data Preprocessing: Ensuring image data is in the appropriate format for training.
Model Architecture: Designing a deep learning architecture suitable for ASL sign recognition.
Training and Validation: Iterative training and validation loops for optimal model performance.
Fine-tuning: Adjusting hyperparameters to achieve high accuracy.
2. Live Detection
Real-time Processing: Efficient video processing techniques for live camera input.
Model Inference: Utilizing the pre-trained model to classify signs in each frame.
Display Output: Showing the detected sign with associated information on the live feed.
